# 3부. 데이터 분석하기

## 13. 머신러닝 기반 분석 방법론-1
```
[목표]
* 회귀분석 (선형/로지스틱)
* 의사결정나무
* 랜덤 포레스트
* 판별분석
* 서포트벡터머신
```

### 13.1. 선형 회귀분석과 Elastic Net(예측모델)

회귀분석은 각 독립변수의 평균을 통해 종속변수를 예측한다. 회귀분석은 종속변수의 값에 영향을 주는 독립변수들의 조건을 고려하여 구한 평균값으로 정의할 수 있다.

$$y=\beta_0+\beta_1X_1+...+\beta_nX_n+\varepsilon$$

```
y: 종속변수, 예측하고자 하는 값
beta: 절편, 각 독립변수가 종속변수에 주는 영향력 값
X: 독ㄼ변수
error: 잔차, 모델에 의해 설명되지 않는 부분
```

최적의 회귀선을 구하는 것을 **모형 적합**이라고 하며 회귀선과 각 관측치를 뜻하는 점 간이 거리를 최소화한다. => 최조제곱추정법(LSE)

```
[회귀분석 시 주의할 점]
- 독립변수 간 상관관계가 없어야 함 -> 다중공선성 검사
- 잔차의 정규성
- 잔차의 등분산성
- 선형성: 독립변수 값의 변화에 따른 종속변 값의 변화는 일정해야 한다. 만일 비선형적인 관계를 갖고 있을 경우 회귀선의 예측력이 떨어짐.
```

```
[다항회귀]
- 독립변수와 종속변수의 관계가 비선형 관계일 때 변수에 각 특성의 제곱을 추가하여 회귀선을 비선형으로 변환
- 차수가 커질수록 편향은 감소하지만 변동성이 증가 -> 과적합 유발
```

다중 회귀분석 변수별 계수 및 유의도 결과를 확인하여 분석가가 변수 조합을 테스트할 수도 있지만 이는 매우 비효율적임 -> 변수 선택 알고리즘

```
[변수 선택 알고리즘]
# 전진 선택법: 유의미한 독립변수 순으로 변수를 차례로 하나씩 추가
# 후진 제거법: 모든 독립변수가 포함된 상태에서 시작하여 유의미하지 않은 설명변수를 하나씩 제거
# 단계적 선택법: 전진선택법 + 후진제거법 : 처음에는 전진선택법처럼 변수를 하나씩 추가하기 시작하면서, 선택된 변수가 3개 이상이 되면 변수 추가와 제거를 번갈아가며 수행
# 그 밖의 변수 선택 알고리즘 : LARS, 유전자 알고리즘, Elastic Net
```

Elastic Net : 릿지, 라쏘 회귀를 조합한 변수 알고리즘
```
# 릿지
 - 전체 변수를 모두 유지하면서 각 변수의 계수 크기를 조정
 - L2-norm
 - 독립변수들의 영향력 조정: 계수 정규화
 - 매개변수 α값을 조정하여 정규화 수준을 조정
 - 제약조건까지 가는 가장 작은 잔차제곱합의 맞닿는 지점의 파라미터 값이 작게라도 존재
 - 변환된 계수가 0이 될 수 없음
# 라쏘
 - 중요한 몇 개의 변수만 선택하고 나머지 변수들은 계수를 0으로 두어 변수의 영향력을 아예 없앰
 - L1-norm
 - 릿지처럼 α값을 조정하여 정규화의 강도를 조정
 - 파라미터 값의 크기에 상관없이 같은 수준으로 정규화를 하기 때문에 영향력이 작ㅇ느 변수를 모델에서 삭제
 - 변환된 계수가 0이 될 수 있음
```
Elastic Net은 혼합비율(r)을 조정하여 모델의 성능을 최적으로 끌어내고자 함. 독립변수를 이미 잘 정제해서 중요할 것으로 판단되는 변수들만 선별해서 모델에 넣은 상태면 릿지의 비율을 높이는 것이 좋고, 변수 선택 없이 주어진 독립변수를 모두 집어넣은 상태라면 라쏘의 비율을 높이는 것이 좋다.

```
[회귀모델 자체에 대한 평가]
- 모델에 대한 유의도
- 모델 설명력: 선택된 독립변수들로 종속변수를 어느 정도 설명할 수 있는지를 판단
- 수정된 모델 설명력: 독립변수의 수가 많아질수록 설명력이 높아지는 경향성을 막기 위한 기준값
```

```
[p-value와 R^2 값에 따른 모델 튜닝 전략]
# 낮은 p-value와 높은 R^2 -> 이상적, 주요 인자 추출
# 낮은 p-value와 낮은 R^2 -> 이상치 제거, 비선형 회귀분석 적용
# 높은 p-value와 높은 R^2 -> 데이터 확보, 이상치 제거
# 높은 p-value와 낮은 R^2 -> 새로운 변수 탐색, 비선형 회귀분석 적용
```

### 13.2. 로지스틱 회귀분석 (분류모델)

```
[로지스틱 회귀분석]
- 종속변수가 질적척도
- 특정 수치를 예측하는 것이 아니라 어떤 카테고리에 들어갈지 분류하는 모델
```
$$Odds = {P(event\ occurring)\over P(event\ not\ occurring)}$$

오즈는 값의 범위가 0부터 무한대까지라는 한계가 있으므로 오즈에 로그를 취하여 양의 무한대에서 음의 무한대를 값으로 가지는 형태로 변환

$$P = {e^{\beta_0+\beta X}\over{1+e^{\beta_0+\beta X}}} = {1\over{1+e^{-(\beta_0+\beta X)}}}$$

다항 로지스틱 회귀분석의 경우, 하나의 범주를 기준으로 잡고 나머지 다른 범주들과 비교해서 식을 만든다. 따라서 다항 로지스틱 회귀분석은 이항 로지스틱 식이 K-1개가 필요하다.



### 13.3. 의사결정나무와 랜덤 포레스트 (예측/분류 모델)

```
[의사결정나무]
- 나뭇가지들이 뻗어 있는 형태로 데이터들이 분리되어 가며 최적의 예측 조건을 만드는 것
- 가지가 나눠지는 부분은 독립변수의 조건이고, 마지막 잎사귀들은 최종의 종속변숫값들을 나타냄
```

```
[의사결정나무의 종류]
# 분류나무: 명목형 종속변수를 분류
 - 불순도를 낮추고 순도를 높이는 방향으로 분류 기준을 찾아냄 -> '지니계수', '엔트로피'를 기준으로 데이터의 불순도를 나타냄 
 - 노드 내에서는 범주의 동직성이 최대한 높고, 노드 간에는 이질성이 최대한 높도록 만들어주는 것
 - 1회 자식노드를 만들기 위해 변수가 k개, 관측치가 n개라면, k(n-1)번의 계산을 하게 됨

# 회귀나무: 연속형의 수를 예측
 - 잔차 제곱합 등의 분류 기준을 사용
 - 구역을 나누어 값을 예측하기 때문에 종속변수의 비선형성에 영향을 받지 않음
 - 끝 노드에 속한 데이터의 값의 평균을 구해 회귀 예측값을 계산
```

```
[의사결정나무의 장단점]
# 장점
 - 해석의 용이성
 - 비선형 모델이므로 데이터의 선형성, 정규성, 등분산성 등이 필요하지 않음
# 단점
 - 명목형 변수는 예측 데이터에 있는 정보가 학습 데이터에 없으면 예측이 불가능하다 -> 예측할 수 있는 종속변수의 최소 최댓값이 학습 데이터의 범위에 한정되므로 학습 데이터와 예측 데이터의 연속형 변숫값 편차가 큰 경우에는 예측력이 떨어질 수 있음
 - 과적합 될 확률이 높음
 - 학습 성능의 변동이 큼
```

```
[의사결정나무의 과적합 방지를 위한 방법]
- 가지치기 : 가지치기 적정 수준 판별은 분기 가지가 많아질수록 학습 데이터의 오분류율은 낮아지게 되고, 특정 수준 이상이 되면 검증 데이터의 오분류율이 높아지는 원리를 이용
- 정보 획득량 임곗값 설정
- 한 노드에 들어가는 최소 데이터 수 제한하기
- 노드의 최대 깊이 제한하기 등
```

```
[랜덤 포레스트]
- 나무를 여러 개 만들어서 학습
- 앙상블 학습

# 부트스트랩: 하나의 데이터셋을 중복을 허용하여 무작위로 여러 번 추출
# 배깅: 여러 개의 의사결정나무를 하나로 합치는 것
```

### 13.4. 선형 판별분석과 이차 판별분석 (분류모델)

```
[판별분석]
- 로지스틱 회귀분석처럼 질적 척도로 이루어진 종속변수를 분류할 때 사용되는 분석 기법
- 성능면에서 로지스틱 회귀분석보다 우수
- 30% 적은 데이터로도 로지스틱 회귀분석과 유사한 성능을 낼 수 있음
- 독립변수들이 정규분포를 따르지 않더라도 활용 가능
```

```
[판별분석 종류]
# 일반 판별분석: 종속변수의 범주가 두 개
# 다중 판별분석: 종속변수의 범주가 3개 이상

[결정경계선 산출 방식]
# 선형판별분석(LDA)
 - PCA와 같은 차원축소에도 사용됨, PCA와 차이점은 종속변수를 사용하는 지도학습으로 차원축소를 한다는 점
 - 조건: 데이터가 정규분포 / 각 범주들은 동일한 공분산 행렬을 가짐 / 독립변수는 통계적으로 상호 독립

# 이차판별분석(QDA)
 - LDA가 공분산 구조가 많이 다른 범주의 데이터를 잘 분류하지 못한다는 단점을 보완한 방법
 - 비선형 분류가 가능하지만 독립변수가 많을 경우 LDA에 비해 연산량이 크다는 단점
```

### 13.5. 서포트벡터머신 (분류모델)

```
[서포트벡터머신(SVM)]
- 범주를 나눠줄 수 있는 최적의 구분선을 찾아내어 관측치의 범주를 예측
- 이진 분류에만 사용 가능
- 마진을 최대화하는 원리로 결정경계선을 설정
```


---
---

### 13.6. KNN (분류, 예측 모델)

```
[KNN]
- 연속형 종속변수의 회귀예측도 가능
- 기계학습 중 가장 단순한 모델
- 학습 데이터의 별도 학습 과정이 없다 => 메모리 기반 학습
- K개의 근접한 학습 데이터로 분류하는 것, 통상적으로 K는 10 이하
- 최적의 K 찾기: 교차검증을 하여 오분류율이 가장 낮아지는 K수를 탐색
- 거리에 부여하는 가중치 계산: 교차검증을 통해 선택
- 거리 계산: 유클리드 거리로 계산, 정규화나 표준화 처리
- 종속변수가 연속형인 경우에는 회귀 나무처럼 KNN 회귀를 한다. 이 경우, K를 너무 적게 설정하면 학습데이터에 너무 의존된 모델이 되어 일반화가 어려워지므로 안정된 예측 모델을 만들기 위해서는 K를 3 이상으로 설정해 주는 것이 좋음
```

### 13.7. 시계열 분석 (예측 모델)

```
[시계열 분석]
- 관측치의 통계량 변화를 시간의 흐름에 따라 순차적으로 데이터화하고 현황을 모니터링하거나 미래의 수치를 예측하는 분석 방법

# 탐색 목적: 외부 인자와 관련된 계절적인 패턴, 추세 등을 설명하고 인과관계를 규명
# 예측 목적: 과거 데이터 패턴을 통해 미래의 값을 예측
```

```
[회귀 기반 시계열 분석]
- 예측하고자 하는 시점 t의 값이 종속변수가 된다.
- 비선형 추세를 모델에 적합하는 방법
  * 해당 독립변수에 제곱을 취하여 회귀선을 보정
  * 종속변수나 독립변수에 로그를 취해 비선형적 관계를 적합 -> 과적합 위험
- 외부 요소를 변수로 추가해 주는 것이 용이 (부정기적인 노이즈 요소를 최소화 가능)
- 자기회귀요소 변수를 추가하여 자기회귀 요소를 반영할 수 있다. -> ACF
```

```
[ARIMA]
- 이동평균을 누적한 자기회귀를 이용하여 시계열 분석을 하는 방법

# AR, MA 모형을 시작하기에 앞서 시계열 데이터가 '정상성(모든 시점에 대해 일정한 평균을 갖도록 하는 것)'을 가지도록 변환해주어야 한다.
 - 평균이 일정하지 않으면 '차분'을 해준다.
  * 추세만 차분 -> 1차 차분 : 연달아 이어진 관측값들의 차이를 계산
  * 계절성도 존재 -> 2차 차분, 계절차분 : n 시점 전 값의 차이를 계산
 - 분산이 일정하지 않으면 '변환'을 해준다.
  * 각 시점의 값에 로그나 루트를 씌워 분산의 크기를 완화

# AR 모델
 - 회귀 기반 시계열 분석에서 시차 변수만 사용한 개념
 - p 만큼의 과거 데이터를 예측에 이용

# MA 모델
 - 관측값의 이전 시점의 연속적인 예측 오차의 영향을 이용하는 방법
 - q 만큼의 과거 오차 값들을 예측에 이용

# ARMA(p,q)만으로는 모델 자체의 불안정성을 제거할 수 없기 때문에 ARIMA(p,d,q) 사용: 과거의 데이터가 가지고 있던 추세까지 반영, d는 1차 차분이 포함된 정도
```

ARIMA 분석을 하기 전에 우선 시각화와 ACF 차트를 통해 시계열 데이터가 정상시계열인지 확인

시계열 데이터는 검증할 데이터가 적은 편이므로 슬라이딩 윈도우 기법을 사용하여 학습 및 검증 데이터를 증폭시킬 수 있다.


### 13.8. K-MEANS 클러스터링 (군집 모델)

KNN: 지도학습, classification   
vs.   
k-means 클러스터링: 비지도학습, clustering
```
[k-means 클러스터링]
- k는 분류할 군집 수를 의미
  -> <k의 수 결정하기>
     * 비즈니스 도메인 지식을 통한 개수 선정
     * 엘보우 기법
     * 실루엣 계수
- 관측치와 중심점 간의 거리를 이용하기 때문에 데이터의 표준화나 정규화를 꼭 해주어야 한다.
- 중심점과 군집 내 관측치 간의 거리를 비용함수로 하여 이 함수 값이 최소화되도록 중심점과 군집을 반복적으로 재정의
- 단점: 지역 최솟값 문제
  -> 해결법: 초기 중심점 선정 방법을 다양하게 하여 최적의 모델을 선정


# 중심점과 관측치 간의 거리가 아니라 관측치들의 밀도를 통해 자동으로 적절한 군집의 수를 찾을 수도 있음 -> DBSCAN
```





### 13.9. 연관규칙과 협업 필터링 (추천 모델)
### 13.10. 인공 신경망 (CNN, RNN, LSTM)