# 3부. 데이터 분석하기

## 12. 통계 기반 분석 방법론
```
[목표]
* 주성분 분석
* 공통 요인 분석
* Z-TEST
* T-TEST
* ANOVA
* 카이제곱 검정
```

### 12.1. 분석 모델 개요

![방법](/img/5.1.jpg)   
![방법](/img/5.2.jpg)

기계 학습 데이터 분석 방법론은 종속변수의 유무에 따라 크게 지도학습과 비지도학습으로 나눌 수 있다.

지도학습은 입력에 대한 정답이 주어져서 출력된 결괏값과 정답 사이의 오차가 줄어들도록 학습과 모델 수정을 반복한다. 결괏값이 양적 척도면 회귀 방식, 질적 척도면 분류 방식을 사용한다.

비지도학습은 별도의 정답이 없어 변수 간 패턴을 파악하거나 데이터를 군집화하는 방법이다. 크게 차원축소, 군집 분석, 연관규칙 세 가지로 구분지을 수 있다. 차원축소는 주로 지도학습을 할 때 학습 성능을 높이기 위한 전처리 방법으로 사용되는 경우가 많다. 군집 분석의 경우 라벨링 없이 유사한 관측치들끼리 군집으로 분류하는 기법이다. 연관규칙은 경영학에서 장바구니 분석으로 잘 알려져 있는데 제품이나 콘텐츠를 추천하기 위해 사용하는 모델이다.

강화학습은 시행착오를 통해 학습하는 과정을 기본 콘셉트로 한 방법 중 하나이다.

### 12.2. 주성분분석(PCA)

PCA는 여러 개의 독립변수들을 잘 설명해 줄 수 있는 주된 성분을 추출하는 기법이다. 주성분 분석을 통해 전체 변수들의 핵심 특성만 선별하기 때문에 독립변수(차원)의 수를 줄일 수 있다. PCA를 하려면 사용되는 변수들이 모두 등간척도나 비율척도로 측정한 **양적변수**여야 하고 관측치들이 서로 **독립**적이고 **정규분포**를 이루고 있어야 한다.

차원을 감소하는 방법은 첫째, 변수 선택을 통해 비교적 불필요하거나 유의성이 낮은 변수를 제거하는 방법이 있고 둘째, 변수들의 잠재적 성분을 추출하여 차원을 줄이는 방법이 있다. PCA와 CFA는 두번째 방법에 속한다. `PCA`는 변수의 수를 축약하면서 **정보의 손실을 최소화**하고자 할 때 사용되며, `CFA`는 변수들 사이에 존재하는 차원을 규명함으로써 **변수들 간 구조를 파악**하는 데 주로 사용된다.

PCA는 다차원의 데이터 분포를 가장 잘 설명해 주는 성분들을 찾아주는데, 데이터 공간에 위치하는 점들의 **분산을 최대한 보존**하는 축을 통해 차원을 축소하는 것이 핵심 요소이다. 그러한 축을 찾기 전에 우선 데이터 표준화를 해준다. 그 다음 데이터의 분산을 가장 잘 나타낼 수 있는 축을 찾아준다. PCA는 처음 변수의 개수만큼 새로운 성분이 나온다. 각 주성분의 설명력은 전체 분산 중 해당 주성분이 가진 분산으로 표현된다.

### 12.3. 공통요인분석(CFA)

```
[PCA]
- 전체 분산을 토대로 요인을 추출
- 전체 독립변수들을 잘 설명해줄 수 있는 주된 성분 두세 개를 *추출*하는 기법
```
```
[CFA]
- 공통 분산만을 토대로 요인을 추출
- 전체 독립변수를 축약한다는 점에서는 PCA와 동일
- 상관성이 높은 변수들을 *묶어* 잠재된 몇 개의 변수를 찾는다는 점에서 PCA와 차이가 있음
- CFA로 생성한 주성분들은 서로 간 무엇이 더 중요한 변수라는 우위 개념이 없음
```

PCA와 CFA와 같은 요인 분석을 하기 위해서는 우선 독립변수들 간 상관성이 요인분석에 적합한지 검증을 해야한다.(방법: 바틀렛 테스트, KMO 검정)

바틀렛 테스트: 행렬식을 이용하여 카이제곱값을 구해 각 변수 사이의 상관계수의 적합성을 검증하는 방법, P-VALUE < 0.05 이면 대각행렬 아님 -> 변수 간 상관관계 있다고 결론 -> 요인 분석 가능

KMO 검정: 변수 간 상관관계가 다른 변수에 의해 잘 설명되는 정도를 통계적으로 검정, KMO 값이 0.8 이상이면 우수, 0.5 이상이면 적합, 0.5 미만이면 부적합.

적합성을 검증한 후에는 요인분석을 통해 생성되는 주성분 변수들의 **고유치**(요인이 설명해 주는 분산의 양)를 확인하여 **요인의 개수**를 결정한다.

요인 수에 따른 고유치 변화를 그래프로 나타낸 *SCREE PLOT*을 확인하여 ELBOW POINT 까지의 요인을 선택한다.

요인 적재 값을 통해 각 변수와 요인 간의 상관관계 정도를 확인할 수 있다. 요인에 대한 판단은 주관적 판단이 들어갈 수밖에 없으므로 주의해서 해석하고 보편적인 상식에서 벗어나지 않게 해야 한다.



### 12.4. 다중공선성 해결과 섀플리 밸류 분석

다중공선성: 독립변수들 간 상관관계가 높은 현상, 두 개 이상의 독립변수가 서로 선형적 관계를 나타낼 때 다중공선성이 있다고 말한다.

다중공선성이 있다 -> 독립변수 독립 가정 위배 -> 회귀분석에서 모델 정합성 맞지 않는 문제 발생

```
[다중공선성을 판별하는 기준]
# 상관분석
 - 회귀 분석 모델 실행 전 수행
 - 0.7 이상이면 두 변수 간 상관성이 높다고 해석
 - 변수가 많을 경우 상관성을 파악하기 힘들다는 단점

# 회귀분석 결과에서 R^2 값은 크지만 회귀계수에 대한 t값이 낮은 경우 다중공선성 의심
 - t값은 해당 변수의 시그널 강도 -> 클수록 좋음

# VIF(분산팽창계수)
 - 해당 변수가 다른 변수들에 의해 설명될 수 있는 정도
 - 1 부터 무한대의 값을 가짐
 - 10 이상이면 다중공선성 있다고 판단
```

```
[다중공선성을 해결하는 방법]
# VIF값이 높은 변수들 중에서 종속변수와의 상관성이 가장 낮은 변수를 제거하고 다시 VIF값을 확인하는 것을 반복

# 표본 관측치를 추가적으로 확보하여 다중공선성을 완화 -> 비현실적

# 변수를 가공하여 변수 간 상관성을 줄이는 방법
 - 해당 값에 로그를 취하거나, 표준화 및 정규화 변환 등
 - 연속형 변수를 구간화 혹은 명목변수로 변환

# PCA
 - 변수 해석이 어려워진다는 단점

# 변수 선택 알고리즘 활용하기
 - 전진 선택법
 - 후진 제거법
 - 단계적 선택법
 - 분석 경험이 적은 분석가가 활용하기에 좋음
```

섀플리 밸류 분석 : 각 독립변수가 종속변수의 설명력에 기여하는 순수한 수치를 계산하는 방법   
    - 변수 각각의 섀플리 밸류 값을 모두 더하면 모든 변수를 투입했을 때의 설명력과 동일하게 나옴.

### 12.5. 데이터 마사지와 블라인드 분석

데이터 마사지:
- 데이터 분석 결과가 예상하거나 의도한 방향과 다를 때 데이터의 배열을 수정하거나 관점을 바꾸는 등 동일한 데이터라도 해석이 달라질 수 있도록 유도하는 것
- 데이터의 수치 자체를 바꾸는 데이터 조작과 차이가 있음
- 지양해야 함

```
[데이터 마사지 방법]
- 편향된 데이터 전처리
- 매직그래프 사용 -> 절대 사용하면 안됨
- 분모 바꾸기 등 관점 변화
- 의도적인 데이터 누락 및 가공
- 머신러닝 모델의 파라미터 값 변경 및 연산반복
- 심슨의 역설
```

블라인드 분석:
- 데이터 마사지에 의한 왜곡을 방지하기 위해 사용하는 방법
- 편향에 의한 오류를 최소화하기 위한 방법
- 블라인드 분석의 목적은 분석가의 부정행위를 막기 위한 것이 아니라 기존에 분석가가 중요하다고 생각했던 변수가 큰 의미 없는 것으로 결과가 나왔을 때 무리해서 의미부여를 하거나 그 변수에 집착하여 해석에 유리하도록 변수를 가공하게 되는 실수를 방지하고자 하는 데에 있다.
- 점검 보조적 수단으로 사용


### 12.6. Z-TEST와 T-TEST

- 집단 내 혹은 집단 간 평균값의 차이가 통계적으로 유의미한 것인지 알아내는 방법
- 분석하고자 하는 변수: 양적 변수, 정규 분포, 등분산
- Z-TEST -> KNOWN VARIANCE / T-TEST -> UNKNOWN VARIANCE
- 표본의 크기가 30 이상이면 중심 극한 정리에 의해 정규분포를 따른다고 볼 수 있으므로 Z-TEST 사용 가능

단일 집단 평균값 차이를 분석하는 경우, 두 집단의 평균 차이를 분석하는 경우, 비율 차이를 비교하는 경우 예시 설명

### 12.7. ANOVA

- 세 집단 이상의 평균을 검정할 때 사용
- F 분포를 사용
- 독립변수: 범주형 변수 / 종속변수: 연속형 변수
```
# 회귀분석 -> 독립변수, 종속변수 모두 연속형
# 교차분석 -> 독립변수, 종속변수 모두 분류형
```
- 각 집단의 평균값 차이가 통계적으로 유의한지 검정
  집단 평균의 분산이 큰 정도를 따져서 집단 간 평균이 다른지를 판별
- '집단 간 평균의 분산 / 집단 내 분산' 값이 임계치를 초과하는가에 따라 집단 간 평균 차이를 검정 -> 초과하면 통계적으로 유의미한 차이가 있다고 판단
- 사후검증: 어떤 집단 간 차이가 있는지 확인
```
[사후검증]
# 각 집단 수가 같다 -> HSD 검증
# 각 집단 수가 다르다 -> Scheffe 검증
```

### 12.8. 카이제곱검정(교차분석)

- 명목 혹은 서열척도와 같은 범주형 변수들 간의 연관성을 분석하기 위해 결합분포를 활용하는 방법
- 상관분석과 달리 연관성의 정도를 수치로 표현할 수 없음
- 카이제곱 통계량을 통해 변수 간 상관성이 있고 없음을 판단