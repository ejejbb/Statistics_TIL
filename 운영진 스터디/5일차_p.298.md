# 2부. 데이터 분석 준비하기

## 11. 데이터 전처리와 파생변수 생성
```
[목표]
* 데이터 전처리 방법
 - 결측값의 종류와 주요 처리 방법
 - 이상치의 판단 방법과 주요 처리 방법
 - 변수 구간화
 - 데이터 표준화와 스케일링
 - 파생변수 가공
 - 슬라이딩 윈도우 가공
 - 가변수 처리
 - 언더샘플링과 오버샘플링
 - 데이터 거리 측정 방법
```

### 11.1. 결측값 처리

결측값을 처리하기 전 데이터 탐색을 통해 결측값의 비율이 어떻게 되는지, 한 변수에 결측값이 몰려있지는 않은지 등을 파악해야 함. 어떤 경우에는 빈 문자열이 입력되어 있어 결측값으로 인식되지 않을수도 있기 때문에 확인해주어야 한다.

```
[결측치의 종류]

# MCAR 완전 무작위 결측: 순수하게 결측치가 무작위로 발생한 경우
# MAR 무작위 결측: 다른 변수의 특성에 의해 해당 변수의 결측치가 체계적으로 발생한 경우
# NMAR 비무작위 결측: 결측값들이 해당 변수 자체의 특성을 가짐
```

```
[결측치 처리 방법]

# 표본 제거 방법: 결측값이 심하게 많은 변수를 제거하거나 결측값이 포함된 행을 제외하고 데이터 분석을 진행하는 방법
  - 전체 데이터에서 결측값의 비율이 10% 미만인 경우 사용
  - MCAR이 아닌 경우 적절하지 않음

# 평균 대치법: 결측값을 제외한 온전한 값들의 평균을 구한 다음, 그 평균 값을 결측값들에 대치하는 방법
  - MCAR이 아닌 경우 적절하지 않음

# 보간법: 전 시점 혹은 다음 시점의 값으로 대치하거나 전 시점과 다음 시점의 평균값으로 대치하는 방법(단순보간), 결측치가 연달아 있는 경우나 시점 인덱스 간격이 불규칙한 경우에는 선형적 수치값을 계산해 보간함(시점고려보간).
  - 데이터가 시계열적 특성을 가진 경우 효과적

# 회귀대치법: 해당 변수와 다른 변수 사이의 관계성을 고려하여 결측값을 계산, 추정하고자 하는 결측값을 가진 변수를 종속변수로 하고 나머지 변수를 독립변수로 하여 추정한 회귀식을 통해 결측값을 대치
  - EX. '연령' 변수와 '연 수입' 변수 : 연 수입이 높으면 상대적으로 높은 연령을 추정하여 결측값을 대치
  - 결측된 변수의 분산을 과소 추정하는 문제 -> 인위적으로 회귀식에 확률 오차항을 추가하는 '확률적 회귀대치법'을 사용하여 변동성을 조정하기도 함.

# 다중 대치법: 단순대치를 여러 번 수행하여 N개의 가상적 데이터를 생성하여 이들의 평균으로 결측값을 대치하는 방법
  - 대치단계 -> 분석단계 -> 결합단계
  - 대치단계: MCMC 방법이나 MICE 방법을 사용하여 대치값을 임의로 생성, 5개 내외 정도만 생성해도 충분
  - 분석단계: 모수의 추정치와 표준오차 계산
  - 결합단계: 계산된 추정치와 표준오차를 결합하여 최종 결측 대치값 산출
```


### 11.2. 이상치 처리

이상치란 일부 관측치의 값이 전체 데이터의 범위에서 크게 벗어난 아주 작거나 큰 극단적인 값을 갖는 것을 말한다.

```
[이상치 처리를 해야 하는 이유]
- 데이터의 모집단 평균이나 총합을 추정하는 것에 문제를 일으킴
- 분산을 과도하게 증가시켜 분석이나 모델링의 정확도를 감소시킴
```

```
[이상치 처리 방법]
- 이상치를 결측값으로 대체한 다음 결측치 처리
- Trimming: 해당 이상치를 제거
- 관측값 변경: 하한값과 상한값을 정한 후 하한값보다 작으면 하한값으로, 상한값보다 크면 상한값으로 대체
- 가중치 조정: 이상치의 영향을 감소시키는 가중치를 부여

* 통계치를 통한 무조건적인 이상치 탐색은 위험할 수 있다.
  예를 들어 고객의 연령이 225세로 입력되어 있다면 이상치가 아니라 잘못 입력된 데이터인 것을 알고 삭제하면 된다.
  연봉 이상치가 나타나는 경우 분석 모델에 직종(사무직/전문직) 변수를 추가하여 회귀선의 정확도를 높일 수 있다.
```

### 11.3. 변수 구간화

변수 구간화(Binning): 데이터 분석 성능 향상 or 해석의 편리성을 위해 이산형 변수를 범주형 변수로 변환

```
[구간을 나누는 방법]
# 동일 폭으로 변수 구간화
# 동일 빈도로 변수 구간화
# ML 기법 : 클러스터링, 의사결정나무
-> IV값이 높을수록 종속변수의 True와 False를 잘 구분할 수 있는 정보량이 많다는 뜻(0.3보다 크면 예측력이 우수)

[평활화 방법]
# 구간별 평균값으로 평활화
# 구간별 중앙값으로 평활화
# 구간별 경곗값으로 평활화
```

### 11.4. 데이터 표준화와 정규화 스케일링

표준화, 정규화 스케일링: 독립변수들이 서로 단위가 다르거나 편차가 심할 때 값의 스케일을 일정한 수준으로 변환하는 것   
-> 머신러닝 모델의 학습 효율을 증가시키기 때문에 많이 사용

[표준화]   
**StandardScaler**   
각 관측치의 값이 전체 평균을 기준으로 어느 정도 떨어져 있는지 나타낼 때 사용
$$z = {x-\mu\over σ}$$

[정규화]   
**MinMaxScaler**    
데이터의 범위를 0부터 1까지로 변환하여 데이터 분포를 조정하는 방법
$$x_{scaled}={x-x_{min}\over x_{max}-x_{min}}$$

[RoburtScaler]   
표준화와 정규화는 이상치에 민감하다는 단점을 보완한 스케일링 기법   
평균대신 중앙값을 사용

### 11.5. 모델 성능 향상을 위한 파생 변수 생성

`파생변수`는 데이터의 특성을 이용하여 분석 효율을 높이는 것이므로 무작정 변수를 가공해서 만들면 안되고 데이터의 특성과 흐름을 충분히 파악한 후 아이디어를 얻어서 만드는 것이 효과적이다.

파생변수는 기존의 변수를 활용해서 만들어낸 변수이기 때문에 다중공선성 문제가 발생할 가능성이 높다. 그러므로 파생변수를 만든 다음에는 상관분석을 통해 변수 간의 상관성을 확인해야 한다.

### 11.6. 슬라이딩 윈도우 데이터 가공

슬라이딩 윈도우: 실시간 네트워크 패킷 데이터를 처리하는 기법, 각각의 데이터 조각들이 서로 겹치며 데이터가 전송되는 것이 특징이다. 이로 인해 전체 데이터가 증가하는 원리를 차용한 것이 슬라이딩 윈도우 데이터 가공의 핵심이다.

### 11.7. 범주형 변수의 가변수 처리

dummy variable: 범주형 변수를 0과 1의 값을 가지는 변수로 변환해 주는 것, 연속형 변수만 사용 가능한 분석기법을 사용하기 위함.

범주가 3개 이상인 경우, 범주가 늘어날수록 변수의 수를 늘리면 된다.

가변수가 범주의 수보다 하나 적게 만들어지는 것은 데이터의 효율성 뿐만 아니라 변수 간 독립성을 확보하기 위함이기도 하다.

### 11.8. 클래스 불균형 문제 해결을 위한 언더샘플링과 오버샘플링

분류 모델은 클래스 불균형 문제가 발생하는 경우가 많다. 클래스 불균형 문제가 심하면 우리가 원하는 대로 학습이 이루어지지 않는다. 그 근본적인 이유는 대부분의 분류 모델에서 적은 비중의 클래스를 분류하는 것이 중요하기 때문이다.

이러한 불균형 문제를 해결하는 방법은 두 종류가 있다. 첫번째는 모델 자체에 중요도가 높은 클래스에 정확도 가중치를 주는 것(가중치 밸런싱)이고 두번째는 불균형 데이터 자체에 균형이 맞도록 가공한 다음 모델을 학습하는 것이다.(언더샘플링과 오버샘플링)

`언더샘플링`은 큰 비중의 클래스 데이터를 작은 비중의 클래스 데이터만큼 추출하여 학습시키는 것이고, `오버샘플링`은 적은 비중의 클래스 데이터를 큰 비중의 클래스 데이터의 수만큼 복제하여 학습시키는 것이다.

```
[언더샘플링 기법]
# 랜덤 언더샘플링
: 작은 비중의 클래스와 관측치 비율이 유사해질 때까지 무작위로 큰 비중의 클래스의 관측치를 제거
# EasyEnsemble
: 앙상블 기법
: 큰 비중의 클래스를 N개의 작은 비중의 클래스와 동일한 크기의 데이터셋으로 분리한 후, 작은 비중의 클래스 데이터와 분리한 큰 비중의 클래스 데이터를 바꿔가며 학습시키는 것
# CNN
: 딥러닝 알고리즘 CNN과 관계 없음
: 비중이 큰 클래스의 관측치 중에서 비중이 적은 클래스와 *속성값이 확연히 다른 관측치들은 제거*하여 굳이 학습에 사용하지 않아도 되는 관측치를 제거하는 것
```

```
[오버샘플링 기법]

오버샘플링을 적용할 때에는 과적합을 방지하기 위해 학습 셋과 테스트 셋을 분리한 다음에 적용해야 한다.

# 랜덤 오버샘플링
: 작은 클래스의 관측치를 단순 무작위 선택하여 반복 추출하는 방식, 정보의 양이 증가하지 않고 모델 과적합이 발생할 수 있음
# SMOTE
: 대표적인 오버샘플링 기법, KNN 기법 사용
# ADASYN
: SMOTE 기법을 발전시킨 방식
: 오버샘플링할 관측치의 양을 체계적으로 조절 가능
```

### 11.9. 데이터 거리 측정 방법

공간상 데이터들 간의 거리가 가까우면 가까울수록 유사하다고 볼 수 있기 때문에 데이터의 거리를 측정하는 것은 데이터 유사도 측정이라고도 할 수 있다. 때문에 분류모델이나 군집모델의 거의 필수적으로 데이터 거리를 활용한다. 데이터 거리를 측정하기 전에 데이터 표준화나 정규화 가공을 해줘야 한다.

[대표적인 거리 측정 방법]

1. 유클리드 거리
  * 피타고라스 정리를 활용한 것
  * n차원의 데이터에 대한 유클리드 거리 계산 방법
  $$d(A,B) = \sqrt{\sum_{i=1}^n (a_i-b_i)^2}$$

2. 맨해튼 거리
  * 택시 거리라고도 불림
  * 최단거리로 갈 수 없고 격자의 선으로만 지나갈 수 있을 때 사용
  $$d(A,B) = \sum_{i=1}^n |a_i-b_i|$$

3. 민코프스키 거리
  * 옵션값을 설정하여 거리 기준을 조정할 수 있는 거리 측정 방법
  $$d(A,B) = ({\sum_{i=1}^n |a_i-b_i|^p})^{1/p}$$

4. 체비쇼프 거리
  * 민코프스키 거리의 p값을 무한대로 설정한 경우
  * 군집 간 최대 거리를 구할 때 사용
  $$d(A,B) = max(|a_i-b_i|) = \lim_{n→∞}[{\sum_{i=1}^n |a_i-b_i|^n}]^{1/n}$$

5. 마할라노비스 거리
  * 유클리드 거리에 공분산을 고려한 거리 측정 방법
  * 확률분포를 고려
  * $\sum^{-1}$은 공분산 행렬이고 T는 변환행렬
  $$d(A,B) = \sqrt{(A-B)^{\sum^{-1}}(A-B)^T}$$
  ![마할라노비스 거리](/img/4.1.jpg)

6. 코사인 거리
  * 코사인 유사도
    - 벡터 사이의 각도를 구해 두 점 간의 유사도를 측정
    - 두 점 간의 각도가 작으면 유사도가 높고, 각도가 크면 유사도가 낮아짐
    - 일반적으로 0에서 1 사이의 값을 가짐
    - 1 이면 두 점 간 각도가 작음, 유사도가 매우 높음
    - 변수 간의 크기가 중요하지 않을 때 적합
    $$cos(\theta) = {AB \over ||A||||B||}$$
  * 코사인 거리 = 1 - 코사인 유사도